{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import json\n",
    "import itertools\n",
    "import functools\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import munkres\n",
    "import generator\n",
    "import sklearn.cluster"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "DB, labels = generator.generate_subspacedata(2000, 100, False, [[5, 4, 1, 0.1],\n",
    "                                                               [5, 6, 1, 0.1],\n",
    "                                                               [5, 9, 1, 0.1],\n",
    "                                                               [5, 9, 1, 0.1],\n",
    "                                                               [5, 9, 1, 0.1]])\n",
    "np.savetxt(\"res/sample6.csv\", DB, delimiter=\",\", fmt='%1.8f')\n",
    "\n",
    "def persist_ground_truth():\n",
    "    # generates the corresponding ground_truth.csv file from the labels matrix\n",
    "    with open(\"res/ground_truth.csv\", mode=\"w\") as output_file:\n",
    "        cluster_counter = 1\n",
    "        while np.argwhere(labels == cluster_counter).size > 0:\n",
    "            dimensions = list(set(np.argwhere(labels == cluster_counter)[:, 1]))\n",
    "            U = list(set(np.argwhere(labels == cluster_counter)[:, 0]))\n",
    "            cluster_counter += 1\n",
    "            output_file.write(str(dimensions) + \"-\" + str(U) + \"\\n\")\n",
    "persist_ground_truth()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def subscale(epsilon, minpts):\n",
    "\n",
    "    # get the start time\n",
    "    st = time.time()\n",
    "\n",
    "    # the folder \"/out\" must be deleted before each subscale run\n",
    "    dirpath = pathlib.Path('out')\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "\n",
    "    SAMPLEFILE = '6'\n",
    "    GERMAN_FILE_FORMATTING = 'true'\n",
    "    SPLITTING_SIZE = '16'\n",
    "    EVENLY_SIZED_SLICES = 'true'\n",
    "    DBSCAN = 'false'\n",
    "    p = subprocess.Popen(['java',\n",
    "                          '-jar',\n",
    "                          'SubScaleExtended.jar',\n",
    "                          SAMPLEFILE,\n",
    "                          GERMAN_FILE_FORMATTING,\n",
    "                          str(epsilon),\n",
    "                          str(minpts),\n",
    "                          SPLITTING_SIZE,\n",
    "                          EVENLY_SIZED_SLICES,\n",
    "                          DBSCAN],\n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.STDOUT)\n",
    "    for line in p.stdout:\n",
    "        # fix this when java project persists\n",
    "        pass\n",
    "        #print(str(line, \"utf-8\").replace('\\r\\n', ''))\n",
    "    print(\"Duration: \", time.time() - st, \" seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "def dbscan(eps):\n",
    "    \"\"\"\n",
    "    Potential subspaces will be searched for clusters using the dbscan algorithm.\n",
    "    The input subspaces are in a csv format. The clusters found are also written to a csv file.\n",
    "    The algorithm is density-based and is able to recognize multiple clusters. Noise points are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the start time\n",
    "    st = time.time()\n",
    "\n",
    "    with open(\"out/results/found_clusters.csv\", mode=\"w\") as output_file,\\\n",
    "            os.scandir(\"out/results/subspaces\") as it:\n",
    "\n",
    "        # Iterate over the csv files.\n",
    "        for entry in it:\n",
    "            if os.path.getsize( entry.path) == 0:\n",
    "                # Check for empty file sometimes generated due to bug in SubScaleExtended.jar\n",
    "                break\n",
    "            df = pd.read_csv(entry.path, header=None, delimiter=\"-\")\n",
    "\n",
    "            # Iterate over all potential subspaces\n",
    "            for _, row in df.iterrows():\n",
    "                points_indexes = json.loads(row[1])\n",
    "                dimensions = json.loads(row[0])\n",
    "                S = DB[points_indexes][:, dimensions]\n",
    "                clustering = sklearn.cluster.DBSCAN(eps=eps).fit(S)\n",
    "\n",
    "                # Converting dbscan result in clusters.csv file-format\n",
    "                unique_labels = set(clustering.labels_)\n",
    "                unique_labels.discard(-1)  # -1 stands for noisy samples and is therefore removed\n",
    "                for k in unique_labels:\n",
    "                    U = np.array(points_indexes)[clustering.labels_ == k]\n",
    "                    output_file.write(str(dimensions) + \"-\" + str(U.tolist()) + \"\\n\")\n",
    "\n",
    "    print(\"Duration: \", time.time() - st, \" seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def f1(c_res, c_ground):\n",
    "    try:\n",
    "        recall = len(c_res.intersection(c_ground)) / len(c_ground)\n",
    "        precision = len(c_ground.intersection(c_res)) / len(c_res)\n",
    "        return (2 * recall * precision) / (recall + precision)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "def f1_recall(ground, res):\n",
    "    \"\"\"\n",
    "\n",
    "    :param ground:\n",
    "    :param res:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    accumulated_scores = 0\n",
    "    for c_ground in ground:\n",
    "        best_score = 0\n",
    "        for c_res in res:\n",
    "            best_score = max(f1(c_ground, c_res), best_score)\n",
    "        accumulated_scores += best_score\n",
    "    try:\n",
    "        return accumulated_scores / len(ground)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "def rnia(size_union, size_intersection):\n",
    "    \"\"\"\n",
    "    Relative NonIntersecting Area (RNIA)\n",
    "    Formula: 1 - (size_union - size_intersection)/size_union\n",
    "\n",
    "    :param size_union: Cardinality of the union between all micro_objects from both ground_truth and found_clusters.\n",
    "    :param size_intersection: Cardinality of the intersection between all micro_objects from both ground_truth and found_clusters.\n",
    "    \"\"\"\n",
    "    return 1 - (size_union - size_intersection)/size_union\n",
    "\n",
    "def cluster_error(size_union, ground, res):\n",
    "    \"\"\"\n",
    "    The basic idea of cluster_error (CE) is to find a 1:1 mapping between the ground_truth and found clusters.\n",
    "    For each mapped pair (Cg , Cr ) the cardinality of their intersecting micro_objects is determined.\n",
    "    Overall, those 1:1 mappings are chosen that result in the highest total sum over all cardinalities.\n",
    "    This sum is denoted as D_max.\n",
    "    The formula to calculate cluster_error is: 1 - (size_union - D_max)/size_union\n",
    "    \"\"\"\n",
    "\n",
    "    # M is the confusion matrix between the found_clusters and ground_truth clusters. The values\n",
    "    # of the individual entries result from the cardinality of the intersection of the respective clusters.\n",
    "    M = pd.DataFrame(itertools.product(ground,res)).apply(lambda x: len(set(x[0]).intersection(set(x[1]))), axis=1).to_numpy().reshape(len(ground),len(res))\n",
    "\n",
    "\n",
    "    # We use the Hungarian method to find a permutation of the cluster labels such\n",
    "    # that the sum of the diagonal elements of M is maximized.\n",
    "    indexes = munkres.Munkres().compute(munkres.make_cost_matrix(M))\n",
    "    D_max = 0\n",
    "    for row, column in indexes:\n",
    "        value = M[row][column]\n",
    "        D_max += value\n",
    "\n",
    "    return 1 - ((size_union - D_max) / size_union)\n",
    "\n",
    "def score(metric=\"f1\"):\n",
    "    with open(\"out/results/found_clusters.csv\", mode=\"r\") as found_clusters_file,\\\n",
    "            open(\"res/ground_truth.csv\", mode=\"r\") as ground_truth_file:\n",
    "\n",
    "        # A dataframe read from the csv format is converted in such a way\n",
    "        # that the dimensions and point_indexes are available as a set in the corresponding columns.\n",
    "        # Each row is a cluster with column 0 as the dimensions and column 1 as the point_indexes.\n",
    "        df_found_clusters = pd.read_csv(found_clusters_file, header=None, delimiter=\"-\")\\\n",
    "            .apply(lambda s: s.apply(lambda x: (set(json.loads(x)))))\n",
    "        df_ground_truth = pd.read_csv(ground_truth_file, header=None, delimiter=\"-\")\\\n",
    "            .apply(lambda s: s.apply(lambda x: (set(json.loads(x)))))\n",
    "\n",
    "        if metric==\"f1\":\n",
    "            return f1_recall(df_ground_truth[1], df_found_clusters[1])\n",
    "\n",
    "        # Clustering with Cartesian product of dimensions and point_indexes as cluster\n",
    "        # representation. Once for ground_truth, once for found_clusters.\n",
    "        clusters_cartesian_product_ground = [list(itertools.product(x,y)) for x,y in zip(df_ground_truth[0], df_ground_truth[1])]\n",
    "        clusters_cartesian_product_res = [list(itertools.product(x,y)) for x,y in zip(df_found_clusters[0], df_found_clusters[1])]\n",
    "\n",
    "        # Entirety of all micro_objects (cartesian products) of the clustering\n",
    "        # in the form of a set. Once for ground_truth, once for found_clusters.\n",
    "        micro_objects_ground = set(functools.reduce(lambda a, b: a+b, clusters_cartesian_product_ground))\n",
    "        micro_objects_res = set(functools.reduce(lambda a, b: a+b, clusters_cartesian_product_res))\n",
    "\n",
    "        # U is union between all micro_objects from both ground and res\n",
    "        U = micro_objects_ground.union(micro_objects_res)\n",
    "\n",
    "        if metric==\"rnia\":\n",
    "            # I is intersection between all micro_objects from both ground and res\n",
    "            I = micro_objects_ground.intersection(micro_objects_res)\n",
    "            return rnia(len(U), len(I))\n",
    "\n",
    "        if metric==\"ce\":\n",
    "            return cluster_error(len(U), clusters_cartesian_product_ground, clusters_cartesian_product_res)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration:  3.1206486225128174  seconds\n"
     ]
    }
   ],
   "source": [
    "subscale(epsilon=0.2, minpts=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration:  0.05401730537414551  seconds\n"
     ]
    }
   ],
   "source": [
    "dbscan(0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "0.44385026737967914"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(\"ce\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}